{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import utils\n",
    "import scipy.signal as sps\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Ballroom Dataset\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "initialization of _internal failed without raising an exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m     anno_bpm_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBallroom/BallroomAnnotations/ballroomGroundTruth/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m wavfile[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbpm\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m#     anno_bpm_file = 'ISMIR2004/' + genre + '/annotation/' + wavfile[:-4] + ' beat.bpm'\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m y, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39m\u001b[39m{folder}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{file}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(folder \u001b[39m=\u001b[39m folder_path, file \u001b[39m=\u001b[39m wavfile))\n\u001b[0;32m     38\u001b[0m hop_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[0;32m     39\u001b[0m win_length \u001b[39m=\u001b[39m [\u001b[39m172\u001b[39m, \u001b[39m258\u001b[39m, \u001b[39m345\u001b[39m, \u001b[39m431\u001b[39m, \u001b[39m517\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\lazy_loader\\__init__.py:77\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     75\u001b[0m submod_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpackage_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mattr_to_modules[name]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m submod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(submod_path)\n\u001b[1;32m---> 77\u001b[0m attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(submod, name)\n\u001b[0;32m     79\u001b[0m \u001b[39m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m# the module is accessible on the package.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m attr_to_modules[name]:\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\lazy_loader\\__init__.py:76\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m attr_to_modules:\n\u001b[0;32m     75\u001b[0m     submod_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpackage_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mattr_to_modules[name]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m     submod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(submod_path)\n\u001b[0;32m     77\u001b[0m     attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(submod, name)\n\u001b[0;32m     79\u001b[0m     \u001b[39m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[39m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[39m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\librosa\\core\\audio.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msoxr\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlazy_loader\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlazy\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit, stencil, guvectorize\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfft\u001b[39;00m \u001b[39mimport\u001b[39;00m get_fftlib\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconvert\u001b[39;00m \u001b[39mimport\u001b[39;00m frames_to_samples, time_to_samples\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numba\\__init__.py:42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m (cfunc, generated_jit, jit, njit, stencil,\n\u001b[0;32m     39\u001b[0m                                    jit_module)\n\u001b[0;32m     41\u001b[0m \u001b[39m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[0;32m     43\u001b[0m                             get_num_threads, set_num_threads,\n\u001b[0;32m     44\u001b[0m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[0;32m     45\u001b[0m                             get_thread_id)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Re-export Numpy helpers\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy_support\u001b[39;00m \u001b[39mimport\u001b[39;00m carray, farray, from_dtype\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numba\\np\\ufunc\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m Vectorize, GUVectorize, vectorize, guvectorize\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m PyUFunc_None, PyUFunc_Zero, PyUFunc_One\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m _internal, array_exprs\n",
      "File \u001b[1;32mc:\\Users\\tumax\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numba\\np\\ufunc\\decorators.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m _internal\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelUFuncBuilder, ParallelGUFuncBuilder\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m DelayedRegistry\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _internal failed without raising an exception"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Ballroom = ['ChaCha', 'Jive', 'Quickstep', 'Rumba', 'Samba', 'Tango', 'Viennese waltz', 'Waltz']\n",
    "ISMIR2004 = ['Abba', 'Alan_Parsons_Project', 'Alirio Diaz', 'aphex_twin', 'Asian_Dub_Foundation', \n",
    "           'Autechre', 'Aviador_Dro', 'Bach', 'Bach - Walcha', 'Bebel_Gilberto', 'Bela_Bartok',\n",
    "           'Bernstein_conducts_Stravinsky', 'Billie Holiday CD1', 'Bjork', 'Cabaret_Voltaire',\n",
    "           'Carlinhos_Brown', 'charles_mingus', 'Classic', 'Elton Medeiros, Nelson Sargento & Galo Preto',\n",
    "           'Fado', 'Femi_kuti', 'Genesis', 'greek', 'GUITARE+', 'John Frusciante', 'john_coltrane',\n",
    "           'Jose\\' Merce\\'', 'Kocani Orkester', 'Manu_Chao', 'more greek', 'Nina Pastori', 'Olivier Chassain',\n",
    "           'Papakonstantinou', 'Paulinho da Viola & Elton Medeiros', 'Santana', 'Songs', 'Teresa Cristina',\n",
    "           'Tomatito', 'Vangelis', 'Xatzidakis']\n",
    "\n",
    "\n",
    "for ds_id, dataset in enumerate([Ballroom, ISMIR2004]):\n",
    "    \n",
    "    if ds_id == 0:\n",
    "        print('#### Ballroom Dataset')\n",
    "    # else:\n",
    "    #     print('#### ISMIR2004')\n",
    "\n",
    "    for g_id, genre in enumerate(dataset):\n",
    "        \n",
    "        AVG_P_SCORE = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
    "        AVG_ALOTC_SCORE = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
    "\n",
    "        if(ds_id == 0):\n",
    "            folder_path = 'Ballroom/BallroomData/' + genre\n",
    "        # else:\n",
    "        #     folder_path = 'ISMIR2004/' + genre + '/wav'\n",
    "        files = listdir(folder_path)\n",
    "        \n",
    "        for f_id, wavfile in enumerate(files):\n",
    "            \n",
    "            if(ds_id == 0):\n",
    "                anno_bpm_file = 'Ballroom/BallroomAnnotations/ballroomGroundTruth/' + wavfile[:-3] + 'bpm'\n",
    "            # else:\n",
    "            #     anno_bpm_file = 'ISMIR2004/' + genre + '/annotation/' + wavfile[:-4] + ' beat.bpm'\n",
    "\n",
    "            y, sr = librosa.load('{folder}/{file}'.format(folder = folder_path, file = wavfile))\n",
    "            hop_length = 512\n",
    "            win_length = [172, 258, 345, 431, 517]\n",
    "            oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "            tempogram_auto, tempogram_fourier = [], []\n",
    "            T12_AUTO, T12_FOURIER = [], []\n",
    "            s_auto, s_fourier = [], []\n",
    "            auto_frequency, fourier_frequency = [], []\n",
    "            for w_id,wl in enumerate(win_length):\n",
    "                \n",
    "                tempogram_auto = librosa.feature.tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length, win_length=wl, norm=None)\n",
    "                tempogram_fourier = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length, win_length=wl)\n",
    "                \n",
    "                # estimate tempo get T1, T2\n",
    "                tempo_vector_auto = np.sum(tempogram_auto, axis=1)\n",
    "                tempo_vector_fourier = np.sum(np.abs(tempogram_fourier), axis=1)\n",
    "                tempo_vector_auto = [x/tempogram_auto.shape[1] for x in  tempo_vector_auto]\n",
    "                tempo_vector_fourier = [x/tempogram_fourier.shape[1] for x in tempo_vector_fourier]\n",
    "                peak_id_auto = sps.argrelmax(np.array( tempo_vector_auto))[0]\n",
    "                peak_id_fourier = sps.argrelmax(np.array(tempo_vector_fourier))[0]\n",
    "                \n",
    "                no_peak_auto, no_peak_fourier = False, False\n",
    "                if len(peak_id_auto) < 2:\n",
    "                    no_peak_auto = True\n",
    "                if len(peak_id_fourier) < 2:\n",
    "                    no_peak_fourier = True\n",
    "                \n",
    "                auto_frequency = librosa.tempo_frequencies(len( tempo_vector_auto))\n",
    "                fourier_frequency = librosa.fourier_tempo_frequencies(win_length=wl)\n",
    "\n",
    "                if w_id == 0:\n",
    "                    reference_bpm = 0\n",
    "                    with open(anno_bpm_file, 'r') as anno:\n",
    "                        reference_str_bpm = anno.readline()\n",
    "                        reference_bpm = utils.str2float(reference_str_bpm)\n",
    "                \n",
    "                temp = []\n",
    "                if not no_peak_auto:\n",
    "                    for id in peak_id_auto:\n",
    "                        temp.append([ tempo_vector_auto[id], id])\n",
    "                else:\n",
    "                    for i in range(len( tempo_vector_auto)):\n",
    "                        temp.append([ tempo_vector_auto[i], i])\n",
    "                tempo_vector_auto = temp\n",
    "\n",
    "                temp = []\n",
    "                if not no_peak_fourier:\n",
    "                    for id in peak_id_fourier:\n",
    "                        temp.append([tempo_vector_fourier[id], id])\n",
    "                else:\n",
    "                    for i in range(len(tempo_vector_fourier)):\n",
    "                        temp.append([tempo_vector_fourier[i], i])\n",
    "                tempo_vector_fourier = temp\n",
    "                \n",
    "                tempo_vector_auto = sorted( tempo_vector_auto, key=lambda x: x[0], reverse=True)\n",
    "                tempo_vector_fourier = sorted(tempo_vector_fourier, key=lambda x:x[0], reverse=True)\n",
    "                \n",
    "                if no_peak_auto:\n",
    "                     tempo_vector_auto =  tempo_vector_auto[2:]\n",
    "                if no_peak_fourier:\n",
    "                    tempo_vector_fourier = tempo_vector_fourier[2:]\n",
    "\n",
    "                # tempo_vector = [[max_avg_tempogram1, max_id1], [max_avg_tempogram2, max_id2], ...]\n",
    "                T12_AUTO.append([auto_frequency[ tempo_vector_auto[0][1]], auto_frequency[ tempo_vector_auto[1][1]]])\n",
    "                T12_FOURIER.append([fourier_frequency[tempo_vector_fourier[0][1]], fourier_frequency[tempo_vector_fourier[1][1]]])\n",
    "                \n",
    "                \n",
    "                # ALOTC_score\n",
    "                AVG_ALOTC_SCORE[0][w_id] += utils.ALOTC_SCORE(T12_AUTO[w_id], reference_bpm)\n",
    "                AVG_ALOTC_SCORE[1][w_id] += utils.ALOTC_SCORE(T12_FOURIER[w_id], reference_bpm)\n",
    "            \n",
    "        AVG_ALOTC_SCORE = [[score/len(files) for score in AVG_ALOTC_SCORE[0]], [score/len(files) for score in AVG_ALOTC_SCORE[1]]]\n",
    "        print('|{}|4s|8s||12s|'.format(genre))\n",
    "        print('|-----|----------|--------|------------|------------|----------|')\n",
    "        print('|{}|{:6f}|{:6f}|{:6f}|'.format(\"AC\", AVG_ALOTC_SCORE[0][0],  AVG_ALOTC_SCORE[0][2], AVG_ALOTC_SCORE[0][4]))\n",
    "        print('|{}|{:6f}|{:6f}|{:6f}|'.format(\"FOURIER\", AVG_ALOTC_SCORE[1][0], AVG_ALOTC_SCORE[1][2],  AVG_ALOTC_SCORE[1][4]))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
